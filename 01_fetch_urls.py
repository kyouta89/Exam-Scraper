import time
import random
import os
from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- è¨­å®šï¼ˆã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„ï¼‰ ---
TARGET_EXAM = "cis-csm"       # æ¤œç´¢ã—ãŸã„è©¦é¨“åï¼ˆãƒªãƒ³ã‚¯æ–‡å­—ã«å«ã¾ã‚Œã‚‹ã‚‚ã®ï¼‰
CATEGORY_NAME = "servicenow"  # URLã®ä¸€éƒ¨ (discussions/servicenow/)
MAX_PAGE = 150                 # https://www.examtopics.com/discussions/servicenow/ ã®æœ€å¤§ãƒšãƒ¼ã‚¸æ•°
OUTPUT_FILENAME = f'ServiceNow_{TARGET_EXAM}_links.txt'
# ----------------------------------

def init_driver():
    options = webdriver.ChromeOptions()
    # Macã®Chromeã®å ´æ‰€
    options.binary_location = "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome"
    
    # ã‚¹ãƒ†ãƒ«ã‚¹è¨­å®šï¼ˆCloudflareå›é¿ï¼‰
    options.add_argument('--disable-blink-features=AutomationControlled')
    options.add_argument('--no-sandbox')
    options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
    
    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)
    driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
    return driver

def main():
    base_url = f'https://www.examtopics.com/discussions/{CATEGORY_NAME}/'
    all_links = []

    print(f"è©¦é¨“ã€Œ{TARGET_EXAM}ã€ã®URLåé›†ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆå…¨{MAX_PAGE}ãƒšãƒ¼ã‚¸ï¼‰...")
    
    driver = init_driver()

    try:
        # ãƒšãƒ¼ã‚¸ã‚’1ã‹ã‚‰é †ã«å·¡å›
        for page in range(1, MAX_PAGE + 1):
            target_url = f"{base_url}{page}/"
            print(f"[{page}/{MAX_PAGE}] ã‚¢ã‚¯ã‚»ã‚¹ä¸­: {target_url}")
            
            try:
                driver.get(target_url)
                
                # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿ï¼†ãƒ–ãƒ­ãƒƒã‚¯å›é¿ã®ãŸã‚ã®å¾…æ©Ÿ
                time.sleep(random.uniform(5, 8))
                
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                
                # ãƒªãƒ³ã‚¯ã‚’å–å¾— (class="discussion-link" ã‚’æ¢ã™)
                elements = soup.find_all('a', class_='discussion-link')
                
                count_in_page = 0
                for element in elements:
                    link_text = element.get_text().strip()
                    link_href = element.get('href')
                    
                    # è©¦é¨“åãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if TARGET_EXAM.lower() in link_text.lower() and link_href:
                        # ç›¸å¯¾ãƒ‘ã‚¹ãªã‚‰çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                        if not link_href.startswith('http'):
                            link_href = "https://www.examtopics.com" + link_href
                        
                        all_links.append(link_href)
                        count_in_page += 1
                
                print(f"  -> {count_in_page} ä»¶ã®ãƒªãƒ³ã‚¯ã‚’ç™ºè¦‹")

            except Exception as e:
                print(f"  -> ã‚¨ãƒ©ãƒ¼: {e}")

    finally:
        driver.quit()

    # é‡è¤‡ã‚’é™¤å»ã—ã¦ä¿å­˜
    unique_links = sorted(list(set(all_links)))
    
    with open(OUTPUT_FILENAME, 'w', encoding='utf-8') as f:
        for link in unique_links:
            f.write(link + '\n')

    print(f"\nğŸ‰ å®Œäº†ã—ã¾ã—ãŸï¼")
    print(f"åˆè¨ˆ {len(unique_links)} ä»¶ã®URLã‚’ '{OUTPUT_FILENAME}' ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()